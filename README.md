# ROS4PRO : Journ√©e Vision

Ce repository contient le code source √† analyser et compl√©ter pour la journ√©e perception du workshop ROS4PRO. 

*tensorflow** et **keras** sont deux modules Python qui permettent de construire des r√©seaux de neurones apprenants. Nous allons les utiliser pour entra√Æner un r√©seau de neurones √† reconna√Ætre des chiffres √©crits manuellement au feutre avec diff√©rentes calligraphies, ce que l'on appelle aussi **classifier**.

## Pr√©requis

* BAC+2 et +
* Bonne compr√©hension de Python et numpy


## Diapositives

{{<pdf src="https://files.ros4.pro/perception.pdf" >}}

## 1. Documentation

Suivant ton exp√©rience de Python et des modules n√©cessaires, tu pourras utiliser ces ressources :

1. Documentation g√©n√©rale sur numpy :
	* [Numpy cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)
	* [NumPy quickstart](https://numpy.org/devdocs/user/quickstart.html)

2. Pour la partie extraction des faces des cubes et pr√©-processing, nous utiliserons le module `scikit-image`:
	* [Scikit-Image Documentation](https://scikit-image.org/docs/stable)

3. Enfin, pour la classification des images, nous utiliserons le module `keras` inclus dans le module `tensorflow` depuis sa version 2. Un point d'entr√©e sur l'API S√©quentielle de keras peut √™tre consult√© sur cette page :
	* [keras API Sequential](https://www.tensorflow.org/guide/keras/sequential_model?hl=fr)

## 2. Installation

L'entra√Ænement des r√©seaux de neurones avec le module `tensorflow2` se fera de pr√©f√©rence dans un environnement virtuel Python (EVP) qui permet de travailler dans une environnement d√©di√©.
Pour la compr√©hension et la cr√©ation de ton EVP consulte la [FAQ Python : environnement virtuel](https://learn.e.ros4.pro/fr/faq/python_venv/) 

Dans tout le document le _prompt_ du terminal sera not√© `(tf2) jlc@pikatchou $` : le pr√©fixe `(tf2)` est l√† pour bien rappeler que le travail Python se fait 
dans l'__Environnement Virtuel Python tf2__.

üì• Le code source √† t√©l√©charger se trouve [ici](https://github.com/cjlux/ros4pro_perception) : t√©l√©charge l'archive zip, extrait le dossier `ros_perception-master` par exemple dans ton dossier `~/catkin_ws` renome le sous le nom `ros_perception` et installe les paquets Python compl√©mentaires : 

```bash
(tf2) jlc@pikatchou $ cd ~/catkin_ws
(tf2) jlc@pikatchou $ unzip ~/T√©l√©chargements/ros4pro_perception-master.zip 
(tf2) jlc@pikatchou $ mv ros4pro_perception-master ros4pro_perception
(tf2) jlc@pikatchou $ cd ros4pro_perception
(tf2) jlc@pikatchou $ pip install -r requirements.txt
```
## 3. Partie apprentissage

### 3.1 Travail pr√©liminaire avec les notebooks Jupyter üìí

Tape la commande `jupyter notebook` dans le dossier `ros4pro_perception` ; tu peux alors charger les deux notebooks *√† trous* pour la prise en main du *machine learning* avec **tensorflow** et **keras** :

* `notebook/TP1_MNIST_dense.ipynb` : utilise ce notebook pour l'acquisition des bases sur le *machine learning*
	* chargement et utilisation de la banque d'images MNIST utilis√©e pour l'entra√Ænement des r√©seaux,
	* construction, entra√Ænement et exploitation d'un r√©seau de neurones dense conduisant √† un taux de reconnaissance des images MNIST voisin de 98 %.
	
* `notebook/TP2_MNIST_convol.ipynb` : utilise ensuite ce notebook pour la construction d'un r√©seau convolutif, son entra√Ænement avec les images MNIST et son exploitation, conduisant √† un taux de reconnaissance voisin de 99 %.

### 3.2 Travail avec les fichiers Python du dossier `src/`

Une fois familiaris√© avec les principes de construction des r√©seaux denses et convolutifs, tu peux utiliser les programmes Python du r√©pertoire `ros4pro_perception/src/`.

1. Chargement des images MNIST<br>
Ouvre maintenant le fichier `src/learning.py`, prends connaissance du code, puis lance le programme.<br>
Avant d'appuyer sur la touche ENTER, assure-toi que tu sais r√©pondre aux questions :
	* Que contiennent les variables `x_train` et `y_train` ?
	* Pourquoi la fonction `load_data` renvoie-t-elle √©galement les donn√©es `x_test` et `y_test` ?
	* Quelles sont les formes (_shape_) respectives de `x_train` et `y_train` ?

2. Pr√©visualisation des donn√©es brutes<br>
Appuye sur la touche ENTER pour continuer, et observe les images :
	* Quelles sont les valeurs des pixels blancs et noirs ?
	* Observe les donn√©es et leurs labels. Toutes les images sont elles simples √† classifier correctement ?

3. Pr√©paration des donn√©es<br>
Ferme la fen√™tre et appuye √† nouveau sur la touche ENTER :
	* Quelles sont les formes de `x_train` et `y_train` maintenant ?
	* Pourquoi ces changements ?

4. Pr√©visualisation des donn√©es pr√©par√©es<br>
Appuye √† nouveau sur la touche ENTER et observe les images :
	* Quelles sont les valeurs des pixels blanc et noirs maintenant ?
	* Regarde la fonction `prepare_input` : quelle transformation des images est effectu√©e ?

5. Le mod√®le du r√©seau convolutif<br>
	* Arr√™te le script. Dans le fichier source `learning.py` modifie la fonction `build_model` pour impl√©menter un r√©seau convolutif semblable √† celui impl√©ment√© dans le notebook `TP2_MNIST_convol.ipynb`.
	* Relance le script et fait d√©filer jusqu'√† la partie 5) (tu peux modifier `SHOW_SAMPLES` pour ne pas afficher toutes les fen√™tres...) : v√©rifie les informations des couches sur le r√©sum√© du mod√®le...

6. La fonction de co√ªt et l'optimiseur<br>
Arr√™te le script et v√©rifie :
	* la fonction de co√ªt et l'optimiseur utilis√©s dans l'appel √† `modele.compile(...)`
	
7. Entra√Ænement :
	* Observe la fonction `train_model` : v√©rifie la pr√©sence et le param√©trage de la gestion de l'_over-fit_.
	* Relance le code jusqu'au d√©clenchement de la partie 7) : tu devrais voir les it√©rations d'entra√Ænement se succ√©der et s'arr√™ter sur un √©v√©nement __early stopping__.

8. Poids appris<br>
Appuye sur la touche ENTER pour visualiser les noyaux convolutifs appris par le r√©seau de neurones :
	* noyaux de la premi√®re couche : arrives-tu √† distinguer le genre de _features_ qui seront extraites par chacun ?
	* Peux-tu faire de m√™me pour la deuxi√®me couche ?	

9. Activations<br>
Appuye sur la touche ENTER, puis entre un indice (un entier inf√©rieur √† 12000 (pourquoi 1200 ?)) :
	* Apr√®s la premi√®re couche de convolution, les _features_ extraites correspondent-elles √† celles que tu imaginais ?
	* Apr√®s la premi√®re couche de _pooling_, les _features_ pr√©sentes auparavant sont-elles conserv√©es ?
	* Apr√®s la deuxi√®me couche de _pooling_, l'information spatiale est toujours pr√©sente ? Autrement dit, les activations ressemblent elles toujours √† des images ?

10. Entra√Ænement final<br>
Arr√™te le script. Jusqu'√† pr√©sent, nous avons travaill√© sur l'ensemble des images montrant des chiffres de '0' √† '9', mais pour la suite nous n'aurons besoin que des images de '1' et de '2' :
	* Change la valeur de la variable `CLASSES` pour ne garder que les classes qui nous int√©ressent.
	* Change `SHOW_SAMPLES`, `SHOW_WEIGHTS` et `SHOW_ACTIV` pour sauter les affichage graphiques...
	* Entra√Ænne le r√©seau avec le nouveau jeu de donn√©es r√©duites, puis sauvegarde-le en donnant le nom d'un r√©pertoire o√π stocker les fichiers du r√©seau entra√Æn√©.

Tu peux passer maintenant √† la **Partie Vision** qui permettra, une fois achev√©e, d'observer les inf√©rences du r√©seau avec les images des cubes correctement trait√©es...

## 4. Partie Vision

Le but de la partie Vision est de traiter les images fournies par la cam√©ra du robot :

![212.png](img/212.png)

pour trouver les contours des cubes :

![212_contours.png](img/212_contours.png)

et extraire des images compatibles MNIST :

![212_contours.png](img/2.png)

qui seront envoy√©es au r√©seau de neurone pour classification en '1' ou '2'...

### 4.1 Pr√©sentation des donn√©es

Ouvre le fichier `src/detection.py` et lance le script. Une des images exemple issue de la cam√©ra du robot appara√Æt :

* Observe les valeurs de pixels ? Quelles sont les valeurs de pixels blancs et noirs ?

* De mani√®re g√©n√©rale, la face des cubes est-elle semblable aux images MNIST ?

### 4.2 Binarisation de l'image

Appuye sur la touche ENTER pour afficher l'image binaris√©e :

* Peux-tu penser √† un algorithme permettant d'arriver √† un r√©sultat similaire ?

Dans le code, observe la fonction `binarize` :

* √Ä quoi sert la fonction `threshold_otsu` ? (voir au besoin la documentation  `scikit-image`).

En commentant successivement les lignes les utilisant, observe l'impact de chacune des fonctions suivantes :

* `closing`
* `clear_border`
* `convex_hull_object`

Pourquoi faut-il √©viter d'avoir des cubes qui touchent les bords de l'image ?

### 4.3 Recherche des contours des cubes

Appuye sur la touche ENTER pour faire d√©filer quelques images dont les contours ont √©t√© d√©tect√©s.

Observe la fonction `get_box_contours`:

* √Ä quoi sert la fonction `label` ?
* √Ä quoi sert le param√®tre `area` ?
* √Ä quoi sert la fonction numpy `argsort` utilis√©e √† la fin pour le r√©-arragement des contours ?
Pourquoi cette op√©ration est elle importante ?

### 4.4 Extraction des vignettes

Appuye sur la touche ENTER pour faire d√©filer quelques images dont les vignettes ont √©t√© extraites.

Observe la fonction `get_sprites`: qu'est ce qu'une "transformation projective" ?

### 4.5 Pr√©paration des images

Pendant la phase d'apprentissage, nous avons √©tudi√© la pr√©paration qui √©tait faite des images.

Les vignettes pr√©sent√©es au r√©seau de neurones doivent aussi √™tre trait√©es pour avoir les m√™mes caract√©ristiques que les images d'entrainement MNIST :

* compl√®te la fonction `preprocess_sprites` pour effectuer ce traitement...

Une fois fait, ex√©cute le script jusqu'√† la fin et conclue sur l'allure des images trait√©es.

Tu peux maintenant ouvrir le fichier `main.py` pour tester l'int√©gration de la d√©tection et de la reconnaissance par r√©seau apprenant...

## 5. Int√©gration

Il est maintenant temps d'int√©grer les deux parties du pipeline pour l'utilisation finale. Ouvre le fichier `main.py` √† la racine du projet.

Pour que les deux parties du pipeline s'adaptent correctement, tu as compl√©t√© la fonction `preprocess_sprites` pour mettre les vignettes renvoy√©es par la partie d√©tection dans un format compatible avec celui des images MNIST.

Ex√©cute maintenant le programme `main.py` : donne le chemin d'un dossier qui contient les fichiers du r√©seau entra√Æn√© et tu devrais commencer √† obtenir la reconnaissance des chiffres '1' et '2' dans les images fournies.

Il faudra certainement refaire plusieurs fois l'entra√Ænement du r√©seau en jouant sur plusieurs param√®tres avant d'obtenir un r√©seau entra√Æn√© qui fonctionne correctement :

* la valeur de la graine `SEED` peut conduire √† un √©tat initial des poids du r√©seau qui donne un entra√Ænement meilleur ou pas...

* augmenter/diminuer `BATCH_SIZE` peut modifier les temps de calcul et la qualit√© du r√©seau entra√Æn√©...

* augmenter/diminuer le param√®tre `patience` du callback `EarlyStopping`...

* enfin, tous les param√®tres qui d√©finissent les couches de convolution et de __spooling__ du r√©seau convolutif sont autant de possibilit√©s d'am√©liorer (ou pas) les performances du r√©seau entra√Æn√©....

√Ä toi de jouer pour obtenir un r√©seau entra√Æn√© classifiant le mieux possible les chiffres '1' et '2' dans les images fournies par la cam√©ra du robot...

Pour confirmer la qualit√© de ton r√©seau entra√Æn√© tu peux enregistrer tes propres fichiers PNG avec les images faites avec la cam√©ra du robot en utilisant le service ROS `/get_image`. 

Aide-toi des indications du paragraphe __2.4. R√©cup√©rer les images de la cam√©ra en Python__ dans la section [Manipulation/Poppy Ergo Jr](https://learn.ros4.pro/fr/manipulation/ergo-jr/) : tu peux ajouter une instruction `cv2.imwrite(<file_name>, image)` pour √©crire tes propres fichiers PNG dans le r√©pertoire `data/ergo_cubes/perso` et modifier en cons√©quence la variable `img_dir` du fichier `main.py`.

Lance le programme et observe les performances de ton r√©seau op√©rant sur tes propres images.

